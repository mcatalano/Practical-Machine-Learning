---
title: "Human activity prediction from wearable sensor data"
author: "Michael Catalano"
date: "Monday, July 20, 2015"
output: html_document
---

###Summary

A classification model was applied to data generated from wearable fitness devices with the aim of predicting a specific human activity class based on corresponding sensor data. The dataset 'pml-training.csv' contains 19622 observations of 160 variables, most of which are accelerometer output. The 'classe' variable contains the five activity classes A, B, C, D and E, which correspond uniquely to one of the following activities: sitting down, standing up, standing, walking, or sitting. The original data can be viewed here: http://groupware.les.inf.puc-rio.br/har. After cleaning the data by removing sparse columns and non-predictor features, variable importance was evaluated by a near-zero-variance test as well as scaled PCA. It was determined that PCA did not facilitate differentiation of activity classes, nor did it offer enough dimensionality reduction to make it a reasonable preprocessing measure. Alternatively, a random forest model was applied to all predictors without preprocessing. This method afforded an estimated out-of-bag error rate of 1.2% and an actual OOB error of 0.8% when tested on external data. Cross validation was achieved by splitting the original training set into a smaller training set and a validation set (60/40), which was used for model testing and error determination. 

###Data Preparation

The training and test data were loaded into two separate files. Looking at the training set, it was clear that mostly sparse columns were indicated by blanks in the first row of the data. Blanks were coerced to NAs and subsequently omitted by subsetting the training set. Additionally, the first 7 columns contained no accelerometer data, so they were omitted as well. The training dataset was sliced into sub-training and validation sets in a 60/40 split ratio, respectively.

```{r, warning=F, message=F}
#Load required packages and set seed for reproducibility
library(corrplot)
library(ggplot2)
library(caret)
library(randomForest)
library(e1071)
set.seed(333)

#Store data files
big_train <- read.csv('pml-training.csv')
test_class <- read.csv('pml-testing.csv')

#Remove columns with NA or "" in 1st row
first_row <- big_train[1, ]
first_row[first_row == ""] <- NA
NA_names <- names(first_row[ ,is.na(first_row)])
new_train <- big_train[ ,!names(big_train) %in% NA_names]

#Print first 10 remaining column names
names(new_train[1:10])

#Subset data to all reasonable predictors
trimmed_train <- new_train[8:60]

#Split data into training and validation sets
part <- createDataPartition(y=trimmed_train$classe, p=0.6, list=FALSE)
training <- trimmed_train[part, ]
testing <- trimmed_train[-part, ]
```

###Feature Selection

The "near zero variance" test was performed to get an early indication of the importance of variables. As is evident below, none of the 52 variables exhibits near zero variance. Therefore, we cannot eliminate any of the variables without further exploration.

```{r}
#Test for near zero variance
nzv <- nearZeroVar(training, saveMetrics=TRUE)
nzv
```

Principal component analysis (PCA) showed that the dimensionality of the data could be reduced to 12 principal components to capture 80% of the variance in the training data. To capture 95% of variance, 25 components are required. A biplot of the data against PC1 and PC2 shows separation of the data in 5 clearly identifiable clusters. When this same plot is colored by the 'classe' variable, however, it is evident that all activity classes are represented in each of the 5 clusters. Therefore, PCA may not be a worthwhile preprocessing measure.

```{r}
#PCA on training set minus classe variable
princomp <- prcomp(training[ ,1:52], scale=TRUE)

#Summary shows 80% of variance captured by 12 PCs
summary(princomp)

#Plot variances for each component
screeplot(princomp, type='lines', col=2)

#Biplot shows observation scores (points) and feature loadings (arrows) 
biplot(princomp)

#Plot PC1 vs PC2 colored by activity class.
pcp <- qplot(x=princomp$x[ ,1], 
             y=princomp$x[ ,2], 
             color=training$classe,
             xlab='PC1',
             ylab='PC2')
pcp
```

###Model Training

Training a random forest model on the data allows both evaluation of feature importance and estimation of out-of-bag (OOB) error rate. Additionally, there is no need for cross validation when using an RF model since each tree in the forest is constructed from a different bootstrapped sample of the original data. The model below was limited to 50 trees to reduced processing time. Even with a small number of trees, the OOB error rate is estimated to be 1.2%. A feature plot of the four most important features (based on the Gini index) shows relationships between the features that are most valuable in creating decision splits in the model. It appears that the data with respect to these variables still have considerable overlap, but presumably less than features of lower importance.


```{r}
#Train a random forest model on training data
rf.mod <- randomForest(classe ~.,
                       data=training,
                       type='class',
                       importance=TRUE,
                       ntree=50)
rf.mod

#Examine importance of variables
varImpPlot(rf.mod)

#Visualize important variables with feature plot
trellis.par.set(caretTheme())
fp <- featurePlot(x=training[ ,c(1,3,39,41)],
                  y=training$classe,
                  plot='pairs',
                  auto.key = list(columns = 5),)
fp
```

###Prediction

The random forest model was used to predict class values in the testing set, and the accuracy of prediction was evaluated by constructing a confusion matrix. The out of sample error was calculated to be 0.8%, which is less than the internally estimated OOB error in the RF model (1.2%). With the model returning >99% accuracy on external data, the model was then applied to the un-tampered raw test set to generate a vector of predicted activity classes.

```{r}
#Predict outcomes on testing set using RF model
rf.pred <- predict(rf.mod, newdata=testing)

#Evaluate model accuracy with confusion matrix
rf.cm <- confusionMatrix(rf.pred, testing$classe)
rf.cm

#Predict outcomes for the orignal test set file
rf.test <- predict(rf.mod, newdata=test_class)

#Store predicted values as a vector of answers
answers <- as.vector(rf.test)
answers

#Write predicted values to submission text files
write_files <- function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i], file=filename, quote=FALSE, row.names=FALSE, col.names=FALSE)
    }
}

write_files(answers)
```
